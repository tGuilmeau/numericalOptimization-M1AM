{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ac3531-fb71-43d2-9d48-2cc2d9148fac",
   "metadata": {},
   "source": [
    "## Practical session 2: gradient descent and line-search\n",
    "\n",
    "In today's session, we will implement the gradient descent algorithm and use it to solve problems of the form\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^d} f(x).\n",
    "$$\n",
    "The gradient descent algorithm is a first-order algorithm, meaning that it only requires a first-order oracle of $f$. The performance of the gradient descent algorithm depends on only one parameter, called the step size. This step size can be fixed in advance if we have some information about the regularity of $f$, but it can also be adapted along the iterations if not enough information are available. In this practical session, you will implement the gradient descent algorithm with fixed step size as well as some strategies to iteratively adapt the step sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dcc063-89e1-4aac-b411-7191635e921d",
   "metadata": {},
   "source": [
    "### Gradient descent with fixed step size\n",
    "\n",
    "Starting from an initial point $x_0 \\in \\mathbb{R}^d$, the gradient descent algorithm with fixed step size $\\tau > 0$ generates a sequence $\\{x_k\\}$ such that for any $k \\in \\mathbb{N}$,\n",
    "$$\n",
    "x_{k+1} = x_k - \\tau \\nabla f(x_k).\n",
    "$$\n",
    "The stopping criterion for the gradient descent algorithm will generally be an approximate first-order stationarity condition of the form $\\| \\nabla f(x)\\| \\leq \\epsilon$ for some small $\\epsilon > 0$. A maximum number of iterations can also be enforced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9337bb8-35b0-436e-9569-b8c260f15076",
   "metadata": {},
   "source": [
    "> Complete the function `GD` in the file `algorithms.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb80195-a1b2-4e7f-81f5-10b85a3c6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from algorithms import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6d8ff-d41a-4fc3-a2b5-523048e375d0",
   "metadata": {},
   "source": [
    "You will now test the gradient descent algorithm on the objective function $f_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1380e-b437-4736-b077-7a3eb74cfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import problem1 as pb1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa3abc-fec7-4d52-ab5e-7225c148f41f",
   "metadata": {},
   "source": [
    "> Plot the trajectory of the iterates generated by the gradient descent algorithm implemented in the function `GD` as well as the objective function and gradient norm values achieved by the iterates using the functions `level_points_plot` and `plot_obj_normGrad` from the file `utils.py`. Use $x_0 = (4,4)$ with a precision parameter $\\texttt{prec}=10^{-5}$ and a maximum number of iteration $\\texttt{iterMax} = 10^3$. Try the values $\\tau \\in \\{0.1, 0.3, 0.5\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e7208-ff8c-43cf-8604-e91508c281b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37a3e4-c75d-42e2-a4c9-238a1be523af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c83e9-311d-4420-9632-0ffd942b64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbeb6c-0e04-45ea-bbf8-0ad9623380ef",
   "metadata": {},
   "source": [
    "> What is the \"best\" choice of step size among the three step sizes you tried?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9900a9-5c2a-4cf2-aaf5-fbe423ca408a",
   "metadata": {},
   "source": [
    "> What happens if you try to larger values of $\\tau$ (e.g., $\\tau = 0.51$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a7e8d-bb05-42ac-9d8c-8952b4297caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef06ea-0ebc-4623-bae8-d0130b649e90",
   "metadata": {},
   "source": [
    "### Line-search methods\n",
    "\n",
    "As we saw in the previous example, gradient descent with fixed step size is very easy to implement and can convergence very fast on \"easy\" problems if the step size is well chosen. You will see in the lectures that if $\\nabla f$ is $L$-Lipschitz, choosing $\\tau \\leq 1/L$ ensures that the iterates $\\{ x_k \\}$ generated by GD yield a non-increasing sequence $\\{ f(x_k) \\}$ and that $\\nabla f(x_k) \\rightarrow 0$ (these results can be strengthened under additional hypotheses on $f$). \n",
    "\n",
    "An alternative to knowing $L$ is to adapt the step size at every iteration. In this context, we will introduce line-search methods, which allow to achieve this goal. \n",
    "\n",
    "How do line-search methods work? At iteration $k \\in \\mathbb{N}$, the GD algorithm updates the iterate $x_k$ using the update\n",
    "$$\n",
    "x_{k+1} = x_k - \\tau \\nabla f(x_k)\n",
    "$$\n",
    "for some $\\tau > 0$. Then, a natural idea is to consider the function $\\phi_k : (0,\\infty) \\rightarrow \\mathbb{R}$ defined for any $\\tau > 0$ by\n",
    "$$\n",
    "\\phi_k(\\tau) = f(x_k - \\tau \\nabla f(x_k))\n",
    "$$\n",
    "and minimize it. Notice that $\\phi_k(0) = f(x_k)$. Minimizing $\\phi_k$ is generally too costly, so line-search methods only aim at finding $\\tau > 0$ such that\n",
    "$$\n",
    "f(x_k - \\tau \\nabla f(x_k)) \\leq f(x_k) - c \\tau \\| \\nabla f(x_k) \\|^2\n",
    "$$\n",
    "is satisfied for some value $c \\in (0,1)$. This condition is called the Armijo condition and requires the line-search to achieve a sufficient decrease in the function $f$ (controlled by the constant $c$) at every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e8efe-db67-45a5-b856-795eb3853299",
   "metadata": {},
   "source": [
    "The backtracking line-search (also called Armijo line-search) is a straightforward line-search method that allows to find step sizes $\\tau$ satisfying the Armijo condition. Backtracking works as follows:\n",
    "- choose parameters $\\tau_0 > 0$, $c \\in (0,1)$, and $\\rho \\in (0,1)$ and set $\\tau \\leftarrow \\tau_0$.\n",
    "- while $f(x_k - \\tau \\nabla f(x_k)) > f(x_k) + c \\tau \\| \\nabla f(x_k) \\|^2$, update $\\tau \\leftarrow \\rho \\tau$.\n",
    "- once the loop stops, use the final value of $\\tau$ as the step size in the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5902ba0-f416-44b3-b7e3-8050aa930abe",
   "metadata": {},
   "source": [
    "> Implement the gradient descent algorithm with backtracking line-search in the `algorithms.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e609a-d280-4b1b-a1d9-72d32b9005f6",
   "metadata": {},
   "source": [
    "> Run the gradient descent algorithm with backtracking line-search on the function $f_1$ with $\\rho = 0.5$, $c=0.01$, and $\\tau_0 \\in \\{0.1, 1.0, 10.0\\}$. What do you observe and how do you interpret these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e020d2-d2b4-4de1-a304-77195213cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10698e-8a33-43d3-9be4-7fb2f095379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e14201-397a-4339-826e-f76178e17b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181889b9-83cd-4ef0-b923-7875aa062b13",
   "metadata": {},
   "source": [
    "### To go further: Exact line-search for quadratic problems\n",
    "\n",
    "In general, minimizing the function $\\phi_k : \\tau \\longmapsto f(x_k - \\tau \\nabla f(x_k))$ is not worth the effort, so line-search methods only aim at finding a step size $\\tau$ that satisfies a sufficient decrease condition. However, if $f$ is quadratic, that is\n",
    "$$\n",
    "f(x) = \\frac{1}{2} x^\\top A x + b^\\top x + c\n",
    "$$\n",
    "with $A \\succcurlyeq 0$ and symmetric, then the minimizer of $\\phi_k$ can be written in closed-form. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558127e-6ed7-4f8b-a187-a1a0fe650a6a",
   "metadata": {},
   "source": [
    "> Solve $\\phi_k^\\prime(\\tau) = 0$ in $\\tau$ (use that $\\nabla f(x) = Ax + b$ at some point). The solution is the optimal step size for the iteration $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049120b-ce2d-466b-b5f0-e7182e3e45a2",
   "metadata": {},
   "source": [
    "> Implement the gradient descent algorithm with exact line-search for quadratic functions and run it on $f_1$ (you can use that $A$ is the Hessian of $A$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57eb9a4-76b2-478c-bf10-78c49e79f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595fd935-c716-4747-8594-2d30aabac239",
   "metadata": {},
   "source": [
    "### To go further: A more challenging quadratic problem\n",
    "\n",
    "The objective function $f_1$ is a quadratic function which is relatively simple to minimize: its Hessian matrix has a low condition number (ratio of the largest and smallest eigenvalues, this quantity controls the \"elongation\" of the level sets) and the search space is low-dimensional. You are now going to consider the objective function $f_3$, which is a quadratic function in dimension $d=10^3$ whose Hessian has a condition number $\\kappa = 10^6$ (note that these numbers are still small compared to some large-scale optimization problems seen in some applications such as machine learning, image processing, or meteorology, to name a few)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f661b-2b11-4109-9607-4747fa205625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import problem3 as pb3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835b6ca-1320-49b9-b712-cd05cfcb1462",
   "metadata": {},
   "source": [
    "> Try running the gradient descent algorithm with fixed step size $\\tau \\in \\{ 10^{-7}, 10^{-6}, 10^{-5} \\}$, the gradient descent algorithm with backtracking line search, and the gradient descent algorithm with perfect line search to minimize $f_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49584bd-b055-4792-b8ac-90be965753ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f630d-b5b7-42c1-890d-a4c678966eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ceaedf-c17c-4787-a6da-f24b0a4e8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a052ab8-509e-4017-963d-1ea4773b47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e57419-e2c0-4c34-8405-decb77308f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37151db-bac5-41b4-8744-2e8aee5b9fa2",
   "metadata": {},
   "source": [
    "### To go further: Minimizing the Rosenbrock function\n",
    "\n",
    "Only quadratic objective functions have been considered so far. Let us now try to use gradient descent algorithms to minimize the Rosenbrock function $f_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ede46-6a23-45b1-b173-65e6179d8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import problem2 as pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fc291-5502-4dac-b7ac-e427a29a4b1e",
   "metadata": {},
   "source": [
    "> Find a step size $\\tau > 0$ such that the gradient descent algorithm with fixed step size generates a decreasing sequence $\\{ f(x_k)\\}$ of function values when initialized at $(0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a138a16d-fc0d-4b31-bac7-d5079fe3e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b408c-799d-495e-beb5-843a4e0a78ae",
   "metadata": {},
   "source": [
    "> Does this step size still allows to achieve decrease when starting from outside of the \"valley\" of $f_2$, for instance if $x_0 = (-4,-4)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec85c29-86a7-4b8e-bb13-345d642257b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076a99c-cac8-418d-bf06-14d7b8519bf2",
   "metadata": {},
   "source": [
    "> Try to see if backtracking can help in minimizing the Rosenbrock function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271486d0-7046-4fad-8f43-ef2297afb65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
